## 操作系统：ubuntu-2204



## 主机规划





| 主机名  | hadoop-name-node | hadoop-resource-manager | hadoop-secondary-name-node   |
|------|----------------|-----------------------|-------------------|
| IP   | 172.31.3.201   | 172.31.3.202          | 172.31.3.203      |
| HDFS | NameNode       |                       | SecondaryNameNode |
| YARN |                | ResourceManager       |                   |
|      |                |                       |                   |



| 主机名  | hadoop101    | hadoop102    | hadoop103    | ...         | hadoopxxx    |
|------|--------------|--------------|--------------|-------------|--------------|
| IP   | 172.31.3.101 | 172.31.3.102 | 172.31.3.103 | ...         | 172.31.3.xxx |
| HDFS | DataNode     | DataNode     | DataNode     | DataNode    | DataNode     |
| YARN | NodeManager  | NodeManager  | NodeManager  | NodeManager | NodeManager  |



## 准备一个专门用于hadoop 拥有sudo 权限的用户

> 为每一台节点实例，都创建一个叫`hdp` 的用户，且密码也为 `hdp` 后续可以自由变更 密码。
>
> ```shell
> # 添加用户命令，并按提示输入密码，后面的就全部默认直接回车即可
> adduser hdp
> ```
>
> ![image-20240603115909464](00-完全规划.assets/image-20240603115909464.png)
> 
> > 添加sudo 权限
> 
> ```shell
> echo "hdp ALL=(ALL:ALL) ALL" > /etc/sudoers.d/hdp
> ```
> 
> 



## 使用hdp 用户登录并连接到服务器

> 后续的所有操作都只用hdp 即可。
>
> 在这之后所有的操作都换成用户：`hdp`



## 准备安装包

> 将JDK、HADOOP、FLINK 的安装包放到 `${HOME}/software` 目录 
>
> ```shell
> # 将对应的包放到如下目录
> mkdir ${HOME}/software
> ```
>
> ![image-20240602172722440](00-完全规划.assets/image-20240602172722440.png)





## 修改shell 脚本文件所需参数

> 修改脚本文件，按对应的注释修改，需要修改的地方大概有：用户名、密码、还有IP地址。如果DataNode 的数量有不同，则需要做对应的增删
>
> ```shell
> #!/bin/bash
> 
> # 所有主机统一的用户名和密码
> # TODO: suyh - 用户名和密码修改成对应的值
> HADOOP_USER="hdp"
> HADOOP_PWD="hdp"
> 
> # TODO: suyh - IP 需要修改成对应的值，主机名建议不动。
> # Hadoop NameNode 节点的IP
> HADOOP_NN_IP="172.31.3.201"
> HADOOP_NN_HOST="hadoop-name-node"
> # Hadoop ResourceManager 节点的IP
> HADOOP_RM_IP="172.31.3.202"
> HADOOP_RM_HOST="hadoop-resource-manager"
> # Hadoop SecondaryNameNode 节点的IP
> HADOOP_2NN_IP="172.31.3.203"
> HADOOP_2NN_HOST="hadoop-secondary-name-node"
> 
> 
> # 以ip host 格式填充
> # TODO: suyh - IP 需要修改成对应的值，主机名也要唯一。如果有多个则可以继续添加。
> HADOOP_DN_SOURCE=()
> HADOOP_DN_SOURCE+=("172.31.3.101 hadoop101")
> HADOOP_DN_SOURCE+=("172.31.3.102 hadoop102")
> HADOOP_DN_SOURCE+=("172.31.3.103 hadoop103")
> HADOOP_DN_SOURCE+=("172.31.3.104 hadoop104")
> 
> ...
> ```
>
> ![image-20240602172955261](00-完全规划.assets/image-20240602172955261.png)
>
> > 脚本改好后，同步到每一台主机实例上面，要注意在windows 上修改之后的，换行问题
>
> ```shell
> # 为脚本文件添加执行权限
> chmod +x install.sh
> # 执行该脚本
> ./install.sh
> ```
>
> 

## 免密登录

> 脚本执行完成之后，有三台节点会有提示，按照提示，配置ssh 免密登录
>
> 只需要在有提示的机器上面执行即可
>
> ![image-20240602173655798](00-完全规划.assets/image-20240602173655798.png)



## 重启系统

> 重启所有节点主机

## 启动

### 启动HDFS

> 注意：只需要在`hadoop-name-node` 上面启动即可
>
> ```shell
> start-dfs.sh
> ```
>
> ![image-20240602174807717](00-完全规划.assets/image-20240602174807717.png)

### 停止HDFS

> ```shell
> stop-dfs.sh
> ```

### 启动(YARN)

>  ResourceManager

> 规划在hadoop-resource-manager 上面，那么就需要到对应的机器 实例去运行启动脚本
>
> ```shell
> start-yarn.sh
> ```
>
> ![image-20240602174851247](00-完全规划.assets/image-20240602174851247.png)

### 停止YARN

> ```shell
> stop-yarn.sh
> ```

## 查看进程情况

> 通过jps 命令可以查看当前运行的进程情况
>
> ![image-20240602174950866](00-完全规划.assets/image-20240602174950866.png)

### web 页面

- HDFS 提供的NameNode页面

  > http://${hadoop-name-node}:9870
  >
  > 查看在HDFS 上存储的数据信息
  >
  > ![image-20240602175042826](00-完全规划.assets/image-20240602175042826.png)

- YARN 提供的ResourceManager页面

  > http://${hadoop-resource-manager}:8088
  >
  > 查看YARN 上运行的Job 信息
  >
  > ![image-20240602175052794](00-完全规划.assets/image-20240602175052794.png)