## 操作系统：ubuntu-2204



## 主机规划





| 主机名 | hadoopnn     | hadooprm        | hadoop2nn         |
| ------ | ------------ | --------------- | ----------------- |
| IP     | 172.31.3.201 | 172.31.3.202    | 172.31.3.203      |
| HDFS   | NameNode     |                 | SecondaryNameNode |
| YARN   |              | ResourceManager |                   |
|        |              |                 |                   |



| 主机名 | hadoop101    | hadoop102    | hadoop103    | ...         | hadoopxxx    |
| ------ | ------------ | ------------ | ------------ | ----------- | ------------ |
| IP     | 172.31.3.101 | 172.31.3.102 | 172.31.3.103 | ...         | 172.31.3.xxx |
| HDFS   | DataNode     | DataNode     | DataNode     | DataNode    | DataNode     |
| YARN   | NodeManager  | NodeManager  | NodeManager  | NodeManager | NodeManager  |



## 创建用户 hdp，并为其添加sudo 权限

> 每一台节点实例，创建一个相同的用户，密码也相同
>
> ```shell
> adduser hdp
> echo "hdp ALL=(ALL:ALL) ALL" > /etc/sudoers.d/hdp
> ```



## 切换用户hdp

> 使用hdp 用户完整的连接
>
> 在这之后所有的操作都换成用户：`hdp`



```shell
# 所有主机统一的用户名和密码
hadoop_user=hdp
hadoop_pwd=hadoop

# Hadoop NameNode 节点的IP
hadoopnn_ip=172.31.3.201
hadooprm_ip=172.31.3.202
hadoop2nn_ip=172.31.3.203

# 每一个对应上面的IP
hadoopnn_host=hadoopnn
hadooprm_host=hadooprm
hadoop2nn_host=hadoop2nn

# 以ip host 格式填充
#hadoop_dn_source=("172.31.3.101 hadoop101"  "172.31.3.102 hadoop102" "172.31.3.103 hadoop103" "172.31.3.104 hadoop104")

hadoop_dn_source=()
hadoop_dn_source+=("172.31.3.101 hadoop101")
hadoop_dn_source+=("172.31.3.102 hadoop102")
hadoop_dn_source+=("172.31.3.103 hadoop103")
hadoop_dn_source+=("172.31.3.104 hadoop104")


# 解析出来的所有ip 地址
hadoop_dn_ips=()
# 解析出来的所有host 地址
hadoop_dn_hosts=()

# 整个hadoop data node 的节点数量
hadoop_dn_size=${#hadoop_dn_source[@]}
echo "hadoop data node size: ${hadoop_dn_size}"


for dn in "${hadoop_dn_source[@]}"; do
    echo "$dn"
    read -r ip host <<< "$dn"
    echo "ip: $ip"
    echo "host: $host"
    hadoop_dn_ips+=("$ip")
    hadoop_dn_hosts+=("$host")
done

# 输出整个数组
echo "hadoop_dn_hosts: ${hadoop_dn_hosts[@]}"
echo "hadoop_dn_ips: ${hadoop_dn_ips[@]}"

# 遍历数组元素
for ip in "${hadoop_dn_ips[@]}"; do 
    echo "ip: ${ip}"
done
for host in "${hadoop_dn_hosts[@]}"; do 
    echo "host: ${host}"
done

# 以下标的形式遍历数组
for ((i=0; i<$hadoop_dn_size; i++)); do
    echo "Element $i: ip: ${hadoop_dn_ips[i]}, host: ${hadoop_dn_hosts[i]}"
done

```





## 准备安装包

> 将JDK、HADOOP、FLINK 的安装包放到 `~/software` 目录 



## 配置hosts

> 为所有主机配置对应的主机名，让相互之间能通过内网IP互相访问
>
> > sudo vim /etc/hosts
> >
> > 该文件修改之后自动生效，不需要额外做什么操作。
>
> ```txt
> 172.31.3.101 hadoop101
> 172.31.3.102 hadoop102
> 172.31.3.103 hadoop103
> 172.31.3.104 hadoop104
> 172.31.3.105 hadoop105
> 172.31.3.201 hadoopNN
> 172.31.3.202 hadoopRM
> 172.31.3.203 hadoop2NN
> ```
>
> 使用ping 进行测试验证，以防止哪一个写错了。
>
> ```shell
> ping hadoop101 -c 3
> ping hadoop102 -c 3
> ping hadoop103 -c 3
> ping hadoop104 -c 3
> ping hadoop105 -c 3
> ping hadoopNN -c 3
> ping hadoopRM -c 3
> ping hadoop2NN -c 3
> ```

## 配置ssh

> ```shell
> ssh-keygen -t ed25519 -C hadoopNN
> ssh-keygen -t ed25519 -C hadoopRM
> ssh-keygen -t ed25519 -C hadoop2NN
> ```
>
> > 发送到要免密登录的主机
>
> ```shell
> ssh-copy-id hadoopNN
> ssh-copy-id hadoopRM
> ssh-copy-id hadoop2NN
> ssh-copy-id hadoop101
> ssh-copy-id hadoop102
> ssh-copy-id hadoop103
> ssh-copy-id hadoop104
> ssh-copy-id hadoop105
> ```
>
> 

## 安装 JDK

## 安装hadoop

### 自定义配置文件

> 所在目录：$HADOOP_HOME/etc/hadoop
>
> 即：/opt/module/hadoop-3.2.4/etc/hadoop

| 默认配置文件         | 自定义配置文件             |
| -------------------- | -------------------------- |
| [core-default.xml]   | etc/hadoop/core-site.xml   |
| [hdfs-default.xml]   | etc/hadoop/hdfs-site.xml   |
| [yarn-default.xml]   | etc/hadoop/yarn-site.xml   |
| [mapred-default.xml] | etc/hadoop/mapred-site.xml |

- etc/hadoop/core-site.xml

  ```xml
  <configuration>
      <!-- 指定NameNode的地址，hadoop 内网之间进行通信的端口 -->
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://hadoopNN:8020</value>
      </property>
  
      <!-- 指定hadoop数据的存储目录 -->
      <property>
          <name>hadoop.tmp.dir</name>
          <!-- 注意一下这个地方换成对应的目录位置 -->
          <value>/opt/module/hadoop-3.2.4/data</value>
      </property>
  
      <!-- 配置HDFS网页登录使用的静态用户 -->
      <property>
          <name>hadoop.http.staticuser.user</name>
          <value>hdp</value>
      </property>
  </configuration>
  ```

  

- etc/hadoop/hdfs-site.xml

  ```xml
  <configuration>
      <!-- nn(NameNode) web端访问地址，提供给用户使用的页面地址-->
      <property>
          <name>dfs.namenode.http-address</name>
          <value>hadoopNN:9870</value>
      </property>
  	<!-- 2nn(SecondaryNameNode) web端访问地址-->
      <property>
          <name>dfs.namenode.secondary.http-address</name>
          <value>hadoop2NN:9868</value>
      </property>
      <!-- 
      <property>
          <name>dfs.permissions.enabled</name>
          <value>false</value>
          <description>如果为"true"，则在HDFS中启用权限检查;如果为"false"，则关闭权限检查;默认值为"true"。</description>
      </property>
      -->
  </configuration>
  ```

  

- etc/hadoop/yarn-site.xml

  ```xml
  <configuration>
      <!-- 指定MR走shuffle -->
      <property>
          <name>yarn.nodemanager.aux-services</name>
          <value>mapreduce_shuffle</value>
      </property>
  
      <!-- 指定yarn ResourceManager的地址-->
      <property>
          <name>yarn.resourcemanager.hostname</name>
          <value>hadoopRM</value>
      </property>
  
      <!-- 环境变量的继承 -->
      <!-- TODO: suyh - 听说在3.2 版本以上就不需要配置了？？？ -->
      <!--
      <property>
          <name>yarn.nodemanager.env-whitelist</name>
          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
      </property>
      -->
  </configuration>
  
  ```

  

- etc/hadoop/mapred-site.xml

  ```xml
  <configuration>
      <!-- 指定MapReduce程序运行在Yarn上 -->
      <property>
          <name>mapreduce.framework.name</name>
          <value>yarn</value>
      </property>
  </configuration>
  ```

  

### 配置workers

> 所在目录：$HADOOP_HOME/etc/hadoop
>
> 即：/opt/module/hadoop-3.2.4/etc/hadoop/worker
>
> 要注意不要有任何的其他空格或者空行

```txt
hadoop101
hadoop102
hadoop103
hadoop104
hadoop105
```

### 首次运行需要格式化 NameNode

> 注意：只需要格式化NameNode 所在的那台机器实例
>
> ```shell
> hdfs namenode -format
> ```
>
> 初始化完成会多出一个data 目录(存数据的目录) 以及log 目录
>
> 如果要重新格式化，则需要将 data 和log 目录删除之后再执行，否则会报错

## 启动

### 启动HDFS

> 注意：只需要在hadoop101 上面启动即可
>
> 不过，似乎在任意一台机器上面都可以启动呢
>
> $HADOOP_HOME/sbin/start-dfs.sh
>
> 如果启动报错，用户啥的权限啥的，就需要添加一些环境配置
>
> 还有就是JAVA_HOME 的配置

报错示例：

```txt
root@hadoop001:/opt/module/hadoop-3.2.4# ./sbin/start-dfs.sh 
Starting namenodes on [hadoop001]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [hadoop003]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
```

> vim  etc/hadoop/hadoop-env.sh 

```shell
export JAVA_HOME=/usr/local/java/jdk1.8.0_202

# 这里的root 是根据上面的配置来的：hadoop.http.staticuser.user
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

### 停止HDFS

> $HADOOP_HOME/sbin/stop-dfs.sh

#### 成功之后会如配置的一样

- hadoop001

  ```txt
  root@hadoop001:/opt/module/hadoop-3.2.4# jps
  7095 Jps
  6830 DataNode
  6654 NameNode
  ```

- hadoop002

  ```txt
  root@hadoop002:/opt/module/hadoop-3.2.4# jps
  4338 DataNode
  4462 Jps
  ```

- hadoop003

  ```txt
  root@hadoop003:/opt/module/hadoop-3.2.4# jps
  4275 SecondaryNameNode
  4375 Jps
  4136 DataNode
  ```

### 启动(YARN)

>  ResourceManager

> 规划在hadoop002 上面，那么就需要到对应的机器 实例去运行启动脚本
>
> $HADOOP_HOME/sbin/start-yarn.sh

### 停止YARN

> $HADOOP_HOME/sbin/stop-yarn.sh