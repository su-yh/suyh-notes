## 操作系统：ubuntu-2204



## 主机规划





| 主机名 | hadoopnn     | hadooprm        | hadoop2nn         |
| ------ | ------------ | --------------- | ----------------- |
| IP     | 172.31.3.201 | 172.31.3.202    | 172.31.3.203      |
| HDFS   | NameNode     |                 | SecondaryNameNode |
| YARN   |              | ResourceManager |                   |
|        |              |                 |                   |



| 主机名 | hadoop101    | hadoop102    | hadoop103    | ...         | hadoopxxx    |
| ------ | ------------ | ------------ | ------------ | ----------- | ------------ |
| IP     | 172.31.3.101 | 172.31.3.102 | 172.31.3.103 | ...         | 172.31.3.xxx |
| HDFS   | DataNode     | DataNode     | DataNode     | DataNode    | DataNode     |
| YARN   | NodeManager  | NodeManager  | NodeManager  | NodeManager | NodeManager  |



> 当前文档的相关操作都需要root 用户权限，或者有sudo 权限的用户

## 创建一个普通用户(假如用户名为：`hdp`)

> 为每一台节点实例，都创建一个相同的用户，密码也相同
>
> ```shell
> # 添加用户命令，并按提示输入密码，后面的就全部默认直接回车即可
> adduser hdp
> ```
>
> ```txt
> root@iZwz9882fglpfm76m117psZ:~# adduser hdp
> Adding user `hdp' ...
> Adding new group `hdp' (1000) ...
> Adding new user `hdp' (1000) with group `hdp' ...
> Creating home directory `/home/hdp' ...
> Copying files from `/etc/skel' ...
> New password: 
> Retype new password: 
> passwd: password updated successfully
> Changing the user information for hdp
> Enter the new value, or press ENTER for the default
> 	Full Name []: 
> 	Room Number []: 
> 	Work Phone []: 
> 	Home Phone []: 
> 	Other []: 
> Is the information correct? [Y/n] 
> root@iZwz9882fglpfm76m117psZ:~# 
> ```
>
> > 添加sudo 权限
>
> ```shell
> echo "hdp ALL=(ALL:ALL) ALL" > /etc/sudoers.d/hdp
> ```
>
> 



## 切换到用户hdp

> 后续的所有操作都只用hdp 即可。
>
> 在这之后所有的操作都换成用户：`hdp`



## 准备安装包

> 将JDK、HADOOP、FLINK 的安装包放到 `~/software` 目录 
>
> ```shell
> mkdir ~/software
> ```





## 修改配置shell 文件所需参数

> install.sh



## 配置ssh

> ```shell
> ssh-keygen -t ed25519 -C hadoopNN
> ssh-keygen -t ed25519 -C hadoopRM
> ssh-keygen -t ed25519 -C hadoop2NN
> ```
>
> > 发送到要免密登录的主机
>
> ```shell
> ssh-copy-id hadoopNN
> ssh-copy-id hadoopRM
> ssh-copy-id hadoop2NN
> ssh-copy-id hadoop101
> ssh-copy-id hadoop102
> ssh-copy-id hadoop103
> ssh-copy-id hadoop104
> ssh-copy-id hadoop105
> ```
>
> 



### 首次运行需要格式化 NameNode

> 注意：只需要格式化NameNode 所在的那台机器实例
>
> ```shell
> hdfs namenode -format
> ```
>
> 初始化完成会多出一个data 目录(存数据的目录) 以及log 目录
>
> 如果要重新格式化，则需要将 data 和log 目录删除之后再执行，否则会报错

## 启动

### 启动HDFS

> 注意：只需要在hadoop101 上面启动即可
>
> 不过，似乎在任意一台机器上面都可以启动呢
>
> $HADOOP_HOME/sbin/start-dfs.sh
>
> 如果启动报错，用户啥的权限啥的，就需要添加一些环境配置
>
> 还有就是JAVA_HOME 的配置

报错示例：

```txt
root@hadoop001:/opt/module/hadoop-3.2.4# ./sbin/start-dfs.sh 
Starting namenodes on [hadoop001]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [hadoop003]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
```

> vim  etc/hadoop/hadoop-env.sh 

```shell
export JAVA_HOME=/usr/local/java/jdk1.8.0_202
```

### 停止HDFS

> $HADOOP_HOME/sbin/stop-dfs.sh

#### 成功之后会如配置的一样

- hadoop001

  ```txt
  root@hadoop001:/opt/module/hadoop-3.2.4# jps
  7095 Jps
  6830 DataNode
  6654 NameNode
  ```

- hadoop002

  ```txt
  root@hadoop002:/opt/module/hadoop-3.2.4# jps
  4338 DataNode
  4462 Jps
  ```

- hadoop003

  ```txt
  root@hadoop003:/opt/module/hadoop-3.2.4# jps
  4275 SecondaryNameNode
  4375 Jps
  4136 DataNode
  ```

### 启动(YARN)

>  ResourceManager

> 规划在hadoop002 上面，那么就需要到对应的机器 实例去运行启动脚本
>
> $HADOOP_HOME/sbin/start-yarn.sh

### 停止YARN

> $HADOOP_HOME/sbin/stop-yarn.sh