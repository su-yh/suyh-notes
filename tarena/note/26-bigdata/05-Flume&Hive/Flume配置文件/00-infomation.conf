

# 配置一个agent 为其指定一个别名(a1) 这个别名在启动flume 的时候需要用到，与启动相关联

## 为agent(a1) 配置一个或多个source、source、sinks, 并指定一个别名
## 如果有多个，直接用空格分隔
a1.sources = s1
a1.channels = c1 c2
a1.sinks = k1 k2

# 配置source(s1) type=netcat 从网络中获取数据来源。
# 并监听0.0.0.0(老师在这里说只能配置成这样不能配置成127.0.0.1)的8090端口
a1.sources.s1.type = http
a1.sources.s1.bind = 0.0.0.0
a1.sources.s1.port = 8090
## 为source 配置selector，如果不配置默认是复制模式，为下游每一个sink 都发送一份数据
### type=multiplexing 路由模式，按
### header=state 这个是要监控header 中的数据，由这个数据来拆分
### 如果"header:{"state:cn"}" 则将这个数据存到c1 Channel中
####  curl -X POST -d '[{"headers":{"state":"cn"},"body":"hello this is cn data"}]' http://FlumeHadoopalone:8090
### 如果 "header:{"state:us"}" 则将这个数据存到c2 Channel 中
#### curl -X POST -d '[{"headers":{"state":"us"},"body":"hello this is us data"}]' http://FlumeHadoopalone:8090
### 如果 不是上面两种情况那么默认情况下存到c2 Channel 中
a1.sources.s1.selector.type = multiplexing
a1.sources.s1.selector.header=state
a1.sources.s1.selector.mapping.cn = c1
a1.sources.s1.selector.mapping.us = c2
a1.sources.s1.selector.default = c2


#   拦截器
#   Flume有能力在运行阶段修改/删除Event，这是通过拦截器（Interceptors）来实现的
#   也可以自己实现拦截器

## 一般我们用的拦截器是timestamp


# 配置Channel，内丰或者文件两种
a1.channels.c1.type = memory
# 表示在内存中最多存储1000个Event
a1.channels.c1.capacity = 1000
# 表示每个事务给sink(下游)的最大事件数
a1.channels.c1.transactionCapacity = 100

# 配置c2
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100


# 配置Sink 这里测试，直接以日志的形式输出，并在启动的时候控制其输出到控制台
## a1.sinks.k1.type = logger

# 配置流出到HDFS 中
## 如果报错的话，说明缺少相关hadoop 的依赖jar 包，到hadoop 去找到相关的JAR 包拷贝到lib 目录下面
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://FlumeHadoopalone:9000/flume-cn
a1.sinks.k1.hdfs.fileType = DataStream

a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = hdfs://FlumeHadoopalone:9000/flume-us
a1.sinks.k2.hdfs.fileType = DataStream


# source和sink分别绑定到channel
# sink 只能绑定一个channel
a1.sources.s1.channels = c1 c2
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2



