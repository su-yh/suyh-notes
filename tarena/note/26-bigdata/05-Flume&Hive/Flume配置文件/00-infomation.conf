

# 配置一个agent 为其指定一个别名(a1) 这个别名在启动flume 的时候需要用到，与启动相关联

## 为agent(a1) 配置一个或多个source、source、sinks, 并指定一个别名
## 如果有多个，直接用空格分隔
a1.sources = s1 s2 s3 
a1.channels = c1 c2
a1.sinks = k1 k2

# 配置source(s1) type=netcat 从网络中获取数据来源。
# 并监听0.0.0.0(老师在这里说只能配置成这样不能配置成127.0.0.1)的8090端口
a1.sources.s1.type = http
a1.sources.s1.bind = 0.0.0.0
a1.sources.s1.port = 8090


#  路由器
## 为source 配置selector，如果不配置默认是复制模式，为下游每一个sink 都发送一份数据
### type=multiplexing 路由模式，按
### header=state 这个是要监控header 中的数据，由这个数据来拆分
### 如果"header:{"state:cn"}" 则将这个数据存到c1 Channel中
####  curl -X POST -d '[{"headers":{"state":"cn"},"body":"hello this is cn data"}]' http://FlumeHadoopalone:8090
### 如果 "header:{"state:us"}" 则将这个数据存到c2 Channel 中
#### curl -X POST -d '[{"headers":{"state":"us"},"body":"hello this is us data"}]' http://FlumeHadoopalone:8090
### 如果 不是上面两种情况那么默认情况下存到c2 Channel 中
a1.sources.s1.selector.type = multiplexing
a1.sources.s1.selector.header = state
a1.sources.s1.selector.mapping.cn = c1
a1.sources.s1.selector.mapping.us = c2
a1.sources.s1.selector.default = c2

# 监听命令，将命令的输出作为输入
a1.sources.s2.type = exec
a1.sources.s2.command = ping 192.168.220.130

# 监听文件
a1.sources.s3.type = spooldir
a1.sources.s3.spoolDir = /root/hive_zebra


#   拦截器
#   Flume有能力在运行阶段修改/删除Event，这是通过拦截器（Interceptors）来实现的
#   也可以自己实现拦截器

# 拦截器
a1.sources.s1.interceptors = i1 i2 i3 i4 i5 i6
# 时间戳拦截器: 在Event 的headers 中添加一个 'timestamp=1558788388400' 的属性
# 这个时间是收到这个数据的时间，有了这个拦截器之后，我们在写到HDFS 上时就可以按时间来创建目录了
a1.sources.s1.interceptors.i1.type = timestamp
# 主机名拦截器: 在headers 中添加一个'host=10.42.170.247' 的属性
a1.sources.s1.interceptors.i2.type = host
# 静态拦截器: 直接在headers 中添加一个'key=value' 的属性
a1.sources.s1.interceptors.i3.type = static
a1.sources.s1.interceptors.i3.key = addr
a1.sources.s1.interceptors.i3.value = bj
# UUID 拦截器: 在headers 中添加一个'id=b0354de4-3569-4a54-be85-8d42985379ff'
a1.sources.s1.interceptors.i4.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
# 搜索拦截器: 匹配body 中的数据，满足正则的都将被替换成指定的信息
# hello big1812  ==>  hello big****
a1.sources.s1.interceptors.i5.type = search_replace
a1.sources.s1.interceptors.i5.searchPattern = [0-9]
a1.sources.s1.interceptors.i5.replaceString = *
# 正则过滤: 匹配body 中的数据，满足正则的数据都将被过滤掉
a1.sources.s1.interceptors.i6.type = regex_filter
a1.sources.s1.interceptors.i6.regex = ^jp.*$
a1.sources.s1.interceptors.i6.excludeEvents = true
 
## 一般我们用的拦截器是timestamp


# 配置Channel，内丰或者文件两种
a1.channels.c1.type = memory
# 表示在内存中最多存储1000个Event
a1.channels.c1.capacity = 1000
# 表示每个事务给sink(下游)的最大事件数
a1.channels.c1.transactionCapacity = 100

# 配置c2
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100


# 配置Sink 这里测试，直接以日志的形式输出，并在启动的时候控制其输出到控制台
## a1.sinks.k1.type = logger

# 配置流出到HDFS 中
## 如果报错的话，说明缺少相关hadoop 的依赖jar 包，到hadoop 去找到相关的JAR 包拷贝到lib 目录下面
a1.sinks.k1.type = hdfs
# 配合上面的拦截器，自动按天创建目录
# DataStream 文本文件
a1.sinks.k1.hdfs.path = hdfs://FlumeHadoopalone:9000/reportTime=%Y-%m-%d
a1.sinks.k1.hdfs.fileType = DataStream
# 下面的三个条件满足一个就会生成新文件
# 每30s 生成一个新文件
a1.sinks.k1.hdfs.rollInterval = 3600
# 当达到指定文件大小时生成新文件，默认1024 字节  配置成0 就不按大小生成新文件
a1.sinks.k1.hdfs.rollSize = 0
# 按行来生成新文件，配置成0 ，则不按行数来生成新文件
a1.sinks.k1.hdfs.rollCount = 0

a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = hdfs://FlumeHadoopalone:9000/flume-us
a1.sinks.k2.hdfs.fileType = DataStream

# avro 写到下游，需要指定下游接收的IP以及端口
a1.sinks.kTest.type = avro
a1.sinks.kTest.hostname = 192.168.220.150
a1.sinks.kTest.port = 8009


# source和sink分别绑定到channel
# sink 只能绑定一个channel
a1.sources.s1.channels = c1 c2
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2



