# 将Flume 中的数据下游push 到一个hive 对应的表中


# 配置Agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置Source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/vip
#       忽略掉正则匹配的文件
a1.sources.r1.ignorePattern = ^.*\\.tmp$
#   拦截器
a1.sources.r1.interceptors = i1
#       正则提取拦截器: 匹配body 中的数据，满足正则的数据将被提取出来，添加到header 中
#           示例数据：{"create_time":"2019-02-09","before_course_id": "PHP","after_course_id": "TESTING"}
#           示例效果：Event: { headers:{today=2019-02-09} body: 7B 22 63 72 65 61 74 65 5F 74 69 6D 65 22 3A 22 {"create_time":" }
a1.sources.r1.interceptors.i1.type = regex_extractor
a1.sources.r1.interceptors.i1.regex = ^.*"create_time":"(\\d{4}-\\d{2}-\\d{2})".*$
a1.sources.r1.interceptors.i1.serializers = s1
#       将拦截到的匹配的字符串，与 today 组合成一个Key-Value 对，并添加到header中: "today:xxx"
a1.sources.r1.interceptors.i1.serializers.s1.name = today

# 配置Sink
a1.sinks.k1.type = hive
a1.sinks.k1.hive.metastore = thrift://hadoop00:9083
a1.sinks.k1.hive.database = vipdb
a1.sinks.k1.hive.table = vip
a1.sinks.k1.hive.partition = %{today}
a1.sinks.k1.serializer =json

# 配置Channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 1000

# 绑定关系
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
