

- ## 小文件的危害
    - > HDFS 中，产生大量元数据，消耗NameNode 内存。
    - > MapReduce 中，产生大量mapTask(对应大量切片)。   
        > - 每一个MAPTASK 实际 上是一个线程。大量线程对服务器的负载压力增大。


- ## 小文件的处理
    - > Hadoop 中，小文件处理的核心思想：将多个小文件合并成一个大文件。
    - > Hadoop 中，提供了原生的合并小文件的方式: Hadoop Archive
        > - 将多个小文件打成了个har 包
        > - hadoop命令: `hadoop archive -archiveName unio.har -p /txt /`
        >> - 指定自己的名字，指定源文件路径(`-p /txt`) 输出的har 包的路径(`/`)
        > - 打har 包的过程实际上是转化为一个reduce 来执行
        > - 打完har 包之后，原数据不会被删除
        > - 查看har 包中的文件：`hadoop fs -ls har:///unio.har`
        >> - `hadoop fs -cat har:///unio.har/words.txt`





