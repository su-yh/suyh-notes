
scala> val a1 = Array(1, 2, 3, 4)
scala> val rdd1 = sc.parallelize(a1, 2)  将数组转换成一个RDD 并分成两个分区

scala> rdd1.glom.collect   

scala> rdd1.partitions.size     查看分区数
scala> val rdd3 = sc.textFile("file:///home/1.txt", 2)      将文件数据封装成RDD，每个元素一行数据
scala> rdd3.collect
scala> rdd3.flatMap { x => x.split(" ") }.count


// 从HDFS 读取文件
scala> val rdd4 = sc.textFile("hdfs://HBaseAlone:9000/1.txt", 2)
scala> rdd4.collect

// 创建RDD， 指定一个集合以及分区数
scala> val rdd6 = sc.makeRDD(Array(1, 2, 3, 4), 2)

// mapPartitions 按分区处理, 懒操作
scala> val rdd8 = rdd6.mapPartitions { it => 
     | val result = List[Int]()
     | var s = 0
     | while (it.hasNext) {
     | s += it.next
     | }
     | result.::(s).iterator
     | }
// 查看结果
scala> rdd8.glom.collect

// 按分区编号(index)来处理，懒操作
scala> val rdd9 = rdd6.mapPartitionsWithIndex { (index, it) =>
     | val result = List[String]()
     | var sum = 0
     | while (it.hasNext) {
     | sum += it.next
     | }
     | result.::(index + "|" + sum).iterator
     | }


groupByKey 元素必须是二元元组，以Key 做聚合
scala> val rdd17 = sc.makeRDD(List(("bj", 1), ("sh", 2), ("bj", 3), ("sh", 4)), 2)
scala> val rdd18 = rdd17.groupByKey
scala> rdd18.collect

reduceByKey  元素必须是二元元组，第一个元素默认为KEY
scala> val rdd17 = sc.makeRDD(List(("bj", 1), ("sh", 2), ("bj", 3), ("sh", 4)), 2)
scala> val rdd22 = rdd17.reduceByKey {_+_}

sortByKey(true: 升序，false: 降序) 按Key 排序
scala> val rdd25 = sc.makeRDD(List((18, "rose"), (10, "tom"), (25, "jim"), (12, "jary")), 2)
scala> rdd25.collect
scala> val rdd26 = rdd25.sortByKey(true)

join
scala> val rdd28 = sc.makeRDD(List(("cat", 1), ("dog", 2)), 2)
scala> val rdd29 = sc.makeRDD(List(("dog", 3), ("cat", 4)), 2)
scala> val rdd30 = rdd28.join(rdd29)



coalease 调整分区
scala> val rdd31 = sc.makeRDD(List(1, 2, 3, 4), 2)
scala> val rdd32 = rdd31.coalesce(3, true)
scala> rdd32.partitions.size


// takeOrdered  升序排序后，取出前N项
scala> val rdd322 = sc.makeRDD(List(5, 1, 2, 4, 3), 2)
scala> rdd322.takeOrdered(3)
res47: Array[Int] = Array(1, 2, 3)

// top 降序排序后，取出前N项
scala> rdd322.top(3)
res48: Array[Int] = Array(5, 4, 3)

// top 降序排序后，取出前N项 且，指定排序规则
d2.top(3) { Ordering.by { x => x._2 } }

// 将RDD 保存到一个文件中，指定一个目录路径
scala> rdd322.saveAsTextFile("file:///home/r1")

// 按KEY 计数，跟value 无关
scala> val rdd33 = sc.makeRDD(List(("hello", 3), ("hello", 2), ("world", 1), ("hello", 2)), 2)
scala> rdd33.countByKey
res50: scala.collection.Map[String,Long] = Map(hello -> 3, world -> 1)          

// 将RDD 中的数据保存到HDFS 中 , 指定一个目录。其中，一个分区对应一个结果文件
scala> suyhRdd04.saveAsTextFile("hdfs://HBaseAlone:9000/wordresult")


// 将指定目录下的所有文件读取到一个RDD里，RDD[(filePath), (fileText)]
sc.wholeTextFiles("f://data/inverted/*", 2)  



// RDD 缓存(cache、persist)
// unpersist 解除缓存
scala> val r1 = sc.makeRDD(List(1, 2, 3, 4), 2)
scala> val r2 = r1.map { _ * 2 }
scala> r2.cache
scala> r2.persist
scala> import org.apache.spark.storage._
scala> r2.unpersist()
scala> r2.persist(StorageLevel.MEMORY_AND_DISK)
